{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINETUNE, MERGE, AND UPLOAD\n",
    "\n",
    "print(\"Installing required libraries\")\n",
    "!pip install -q -U \"numpy==1.26.4\" \"torch==2.3.1\" \"transformers==4.42.3\" \"peft==0.11.1\" \"accelerate==0.31.0\" \"trl==0.9.4\" \"datasets==2.19.2\" \"bitsandbytes==0.43.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from getpass import getpass\n",
    "BASE_MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "ADAPTER_SAVE_NAME = \"gemma-2-2b-it-numpy-refactor-adapter-v1\"\n",
    "#manually set, then for downloading later\n",
    "HF_REPO_ID = \"priyam-turakhia/gemma-2-2b-it-numpy-refactor-merged-v1\"\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5359a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Python code refactoring tool for NumPy. Your task is to replace only the deprecated functions in the given code snippet with their modern equivalents.\\n\"\n",
    "    \"Your response must be structured with two markdown sections:\\n\"\n",
    "    \"1. A '### Refactored Code' section containing ONLY the updated Python code block.\\n\"\n",
    "    \"2. A '### Deprecation Context' section containing a brief explanation of the deprecation.\\n\"\n",
    "    \"Do not change the code's logic. If no functions are deprecated, return the original code and state that no changes were needed in the context section.\"\n",
    ")\n",
    "\n",
    "print(\"Loading model and tokenizer:\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(\"Preparing dataset\")\n",
    "PATH_TO_TRAINING = 'training_data.json'\n",
    "with open(PATH_TO_TRAINING, 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "\n",
    "def create_prompt(sample):\n",
    "    user_content = f\"{SYSTEM_PROMPT}\\n\\n### INPUT CODE:\\n```python\\n{sample['input']}\\n```\"\n",
    "\n",
    "    assistant_response = (\n",
    "        \"### Refactored Code\\n\"\n",
    "        f\"```python\\n{sample['output']}\\n```\\n\"\n",
    "        \"### Deprecation Context\\n\"\n",
    "        f\"{sample['context']}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"model\", \"content\": assistant_response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "dataset = Dataset.from_list([{'text': create_prompt(s)} for s in training_data])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\" \n",
    ")\n",
    "\n",
    "# LoRA configuration - same target modules work for Gemma\n",
    "print(\"Configuring LoRA and training args\")\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    max_grad_norm=0.3, \n",
    "    save_strategy=\"epoch\",\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=False, \n",
    ")\n",
    "\n",
    "print(\"Fine-tuning:\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving final adapter model to '{ADAPTER_SAVE_NAME}':\")\n",
    "trainer.model.save_pretrained(ADAPTER_SAVE_NAME)\n",
    "print(\"Adapter model saved.\")\n",
    "\n",
    "print(\"Merging LoRA adapter with base model\")\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "base_model_bf16 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "model_merged = PeftModel.from_pretrained(base_model_bf16, ADAPTER_SAVE_NAME)\n",
    "model_merged = model_merged.merge_and_unload()\n",
    "print(\"Adapter merged!\")\n",
    "\n",
    "print(f\"Pushing to '{HF_REPO_ID}':\")\n",
    "model_merged.push_to_hub(HF_REPO_ID)\n",
    "tokenizer.push_to_hub(HF_REPO_ID)\n",
    "print(\"DONE!!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
