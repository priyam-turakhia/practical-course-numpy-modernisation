{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing required libraries\")\n",
    "!pip uninstall -y numpy pandas torch torchvision transformers\n",
    "\n",
    "!pip install --no-cache-dir -q \"numpy>=2.0.0\" \"pandas>=2.2.3\" \"torch>=2.3.0\" \\\n",
    "    \"torchvision>=0.18.0\" \"transformers>=4.42.4\" \"peft==0.11.1\" \"accelerate==0.30.1\" \\\n",
    "    \"trl==0.9.4\" \"datasets==2.19.2\" \"bitsandbytes==0.43.1\" \"numpy_financial\"\n",
    "\n",
    "# RESTART RUNTIME!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import textwrap\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import re\n",
    "import pickle\n",
    "import numpy_financial as npf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "VALIDATION_DATA_PATH = \"validation_data.json\"\n",
    "DETAILED_RESULTS_CSV = \"evaluation_detailed_results.csv\"\n",
    "SUMMARY_METRICS_CSV = \"evaluation_summary_metrics.csv\"\n",
    "\n",
    "EVAL_GLOBALS = {\n",
    "    'np': np,\n",
    "    'ma': ma,\n",
    "    'os': os,\n",
    "    'ast': ast,\n",
    "    'pickle': pickle,\n",
    "    'npf': npf,\n",
    "}\n",
    "\n",
    "HF_BASE_IDS = [\"google/gemma-2-2b-it\", \n",
    "               \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "               \"microsoft/phi-2\", \n",
    "               \"Qwen/Qwen1.5-1.8B-Chat\",\n",
    "               \"Qwen/Qwen2.5-0.5B\"\n",
    "               ]\n",
    "\n",
    "HF_FINETUNED_IDS = [\"priyam-turakhia/gemma-2-2b-it-numpy-refactor-merged-v1\", \n",
    "                    \"julianpins/tinyllama-1.1b-chat-numpy-refactor-v1\", \n",
    "                    \"priyam-turakhia/phi-2-numpy-modernization-v1\",\n",
    "                    \"julianpins/qwen1.5-1.8b-chat-numpy-refactor-merged-v2\",#\n",
    "                    \"julianpins/qwen0.5b-numpy-refactor-v1\"\n",
    "                    ]\n",
    "\n",
    "#select model to evaluate\n",
    "HF_REPO_ID = HF_FINETUNED_IDS[1]\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Python code refactoring tool for NumPy. Your task is to replace only the deprecated functions in the given code snippet with their modern equivalents.\\n\"\n",
    "    \"Your response must be structured with two markdown sections:\\n\"\n",
    "    \"1. A '### Refactored Code' section containing ONLY the updated Python code block.\\n\"\n",
    "    \"2. A '### Deprecation Context' section containing a brief explanation of the deprecation.\\n\"\n",
    "    \"Do not change the code's logic. If no functions are deprecated, return the original code and state that no changes were needed in the context section.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(sample, tokenizer):\n",
    "    if \"phi\" in HF_REPO_ID:\n",
    "        user_prompt = f\"### INPUT CODE:\\n```python\\n{sample['input']}\\n```\"\n",
    "        full_prompt = f\"Instruct: {SYSTEM_PROMPT}\\n\\n{user_prompt}\\nOutput:\"\n",
    "        return full_prompt\n",
    "    \n",
    "    elif \"gemma\" in HF_REPO_ID:\n",
    "        user_content = f\"{SYSTEM_PROMPT}\\n\\n### INPUT CODE:\\n```python\\n{sample['input']}\\n```\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    user_prompt = f\"### INPUT CODE:\\n```python\\n{sample['input']}\\n```\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def parse_model_output(raw_output):\n",
    "    if \"gemma\" in HF_REPO_ID:\n",
    "        output = raw_output.split(\"<start_of_turn>model\")[-1]\n",
    "    elif \"phi\" in HF_REPO_ID:\n",
    "        output = raw_output\n",
    "    else:\n",
    "        output = raw_output.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "        \n",
    "    code_match = re.search(r\"### Refactored Code\\s*```python\\n(.*?)\\n```\", output, re.DOTALL)\n",
    "    context_match = re.search(r\"### Deprecation Context\\n(.*?)$\", output, re.DOTALL)\n",
    "    code = code_match.group(1) if code_match else \"\"\n",
    "    context = context_match.group(1).strip() if context_match else \"\"\n",
    "\n",
    "    if not code and not context:\n",
    "        return output, \"Context not found.\"\n",
    "    return code, context\n",
    "\n",
    "\n",
    "def clean_for_ast(code):\n",
    "    return \"\\n\".join(\n",
    "        line.rstrip() for line in code.splitlines()\n",
    "        if line.rstrip() and not line.strip().startswith(\"#\")\n",
    "    )\n",
    "\n",
    "def compare_outputs(actual, expected):\n",
    "    if isinstance(actual, np.ma.MaskedArray) and actual.ndim == 0: actual = actual.item() if not actual.mask else np.ma.masked\n",
    "    if isinstance(expected, np.ma.MaskedArray) and expected.ndim == 0: expected = expected.item() if not expected.mask else np.ma.masked\n",
    "    if actual is np.ma.masked and expected is np.ma.masked: return True\n",
    "    if isinstance(expected, tuple) and isinstance(actual, tuple):\n",
    "        if len(expected) != len(actual): return False\n",
    "        return all(compare_outputs(a, e) for a, e in zip(actual, expected))\n",
    "    if isinstance(actual, str) and isinstance(expected, str): return actual.replace(\" \", \"\").replace(\"\\n\", \"\") == expected.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "    is_actual_arraylike = isinstance(actual, (np.ndarray, list, tuple))\n",
    "    is_expected_arraylike = isinstance(expected, (np.ndarray, list, tuple))\n",
    "    if is_actual_arraylike and is_expected_arraylike:\n",
    "        try:\n",
    "            actual_arr, expected_arr = np.asarray(actual), np.asarray(expected)\n",
    "            if any(np.issubdtype(arr.dtype, np.number) for arr in [actual_arr, expected_arr]): return np.allclose(actual_arr, expected_arr, equal_nan=True)\n",
    "            return np.array_equal(actual_arr, expected_arr)\n",
    "        except (ValueError, TypeError): return False\n",
    "    if is_actual_arraylike != is_expected_arraylike: return False\n",
    "    return actual == expected\n",
    "\n",
    "def check_compiles(f, code):\n",
    "    execution_scope = {}\n",
    "    try:\n",
    "        exec(code, EVAL_GLOBALS, execution_scope)\n",
    "        compiled_function = execution_scope.get(f)\n",
    "        if not callable(compiled_function):\n",
    "            raise NameError(f\"Function '{f}' was not defined correctly.\")\n",
    "        return True, \"OK\", compiled_function\n",
    "    except Exception as e:\n",
    "        return False, f\"SyntaxError: {e}\", None\n",
    "\n",
    "def check_indentation(f, code):\n",
    "    scope = {}\n",
    "    try:\n",
    "        exec(code, EVAL_GLOBALS, scope)\n",
    "        scope.get(f)\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False , f\"IndentationError: {e}\"\n",
    "\n",
    "def check_no_deprecations(f, input):\n",
    "    if not callable(f): return False, \"Function not callable\"\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        try:\n",
    "            if isinstance(input, tuple): f(*input)\n",
    "            else: f(input)\n",
    "        except AttributeError as e:\n",
    "            if \"module 'numpy' has no attribute\" in str(e):\n",
    "                return False, \"Output contained deprecated features\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error during execution, most likely due to deprecation: {e}\"\n",
    "\n",
    "        for item in w:\n",
    "            if issubclass(item.category, DeprecationWarning):\n",
    "                return False, \"Output contained deprecated features\"\n",
    "    return True, \"OK\"\n",
    "\n",
    "def check_functionality(fun, test_cases: list):\n",
    "    for j, case in enumerate(test_cases):\n",
    "        try:\n",
    "            input = eval(case['input'], EVAL_GLOBALS)\n",
    "            expected_output = eval(case['expected_output'], EVAL_GLOBALS)\n",
    "            actual_output = fun(*input) if isinstance(input, tuple) else fun(input)\n",
    "            if not compare_outputs(actual_output, expected_output):\n",
    "                return False, f\"Failed: [Expected: {repr(expected_output)}, Got: {repr(actual_output)}]\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error during execution of Test Case {j+1}: {e}\"\n",
    "    return True, \"All Test Cases passed\"\n",
    "\n",
    "#evaluation\n",
    "def main():\n",
    "    print(\"Starting evaluation script...\")\n",
    "\n",
    "    print(f\"Loading model from Hugging Face Hub: {HF_REPO_ID}\")\n",
    "    try:\n",
    "        pipe = pipeline(\"text-generation\", model=HF_REPO_ID, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "        tokenizer = pipe.tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading validation data from: {VALIDATION_DATA_PATH}\")\n",
    "\n",
    "    print(f\"VALIDATION_DATA_PATH: {repr(VALIDATION_DATA_PATH)}\")\n",
    "    print(\"Absolute path:\", os.path.abspath(VALIDATION_DATA_PATH))\n",
    "    print(\"File exists:\", os.path.exists(VALIDATION_DATA_PATH))\n",
    "\n",
    "\n",
    "    if not os.path.exists(VALIDATION_DATA_PATH):\n",
    "        print(f\"Validation file not found! Please upload '{VALIDATION_DATA_PATH}'.\")\n",
    "        return\n",
    "    with open(VALIDATION_DATA_PATH, 'r') as f:\n",
    "        validation_data = json.load(f)\n",
    "\n",
    "    results_list = []\n",
    "    total_samples = len(validation_data)\n",
    "\n",
    "    for i, sample in enumerate(validation_data):\n",
    "        print(f\"\\nProcessing sample {i+1}/{total_samples}:\")\n",
    "\n",
    "        prompt = build_user_prompt(sample, tokenizer)\n",
    "        raw_output = pipe(prompt, max_new_tokens=256, do_sample=False)[0]['generated_text']\n",
    "        generated_code, _ = parse_model_output(raw_output)\n",
    "        print(generated_code)\n",
    "        #MOCK\n",
    "        #generated_code = sample['output']\n",
    "\n",
    "        if not generated_code:\n",
    "            print(\"Model did not generate code. Skipping.\")\n",
    "            results_list.append({'sample_index': i, 'compiles': 'Fail: No code generated'})\n",
    "            continue\n",
    "\n",
    "        dedented_code = textwrap.dedent(generated_code).strip()\n",
    "        indented_code = textwrap.indent(dedented_code, \"    \")\n",
    "        full_code = sample['code_before'] + \"\\n\" + indented_code + \"\\n\" + sample['code_after']\n",
    "        full_code_ni = sample['code_before'] + \"\\n\" + generated_code + \"\\n\" + sample['code_after']\n",
    "\n",
    "        function_name = sample['code_before'].split('def ')[1].split('(')[0].strip()\n",
    "        #COMPILATION CHECK\n",
    "        compiles, compiles_msg, compiled_function = check_compiles(function_name, full_code)\n",
    "        #INDENTATION CHECK\n",
    "        indentation, indentation_msg = (check_indentation(f, full_code_ni) if compiles else (False, \"Skipped\"))\n",
    "        #DEPRECATION CHECK\n",
    "        test_input = eval(sample['test_cases'][0]['input'], EVAL_GLOBALS)\n",
    "        no_deprecations, no_deprecations_msg = (check_no_deprecations(compiled_function, test_input) if compiles else (False, \"Skipped\"))\n",
    "        #FUNCTIONALITY CHECK\n",
    "        functionality, functionality_msg = (check_functionality(compiled_function, sample['test_cases']) if compiles and no_deprecations else (False, \"Skipped\"))\n",
    "\n",
    "        results_list.append({\n",
    "            'sample_index': i,\n",
    "            'compiles': 'Pass' if compiles else f'Fail: {compiles_msg}',\n",
    "            'correct_indentation': 'Pass' if indentation else f'Fail: {indentation_msg}',\n",
    "            'no_deprecations': 'Pass' if no_deprecations else f'Fail: {no_deprecations_msg}',\n",
    "            'correct_functionality': 'Pass' if functionality else f'Fail: {functionality_msg}',\n",
    "        })\n",
    "\n",
    "    print(\"\\nEvaluation DONE\")\n",
    "\n",
    "    detailed_df = pd.DataFrame(results_list)\n",
    "    detailed_df.to_csv(DETAILED_RESULTS_CSV, index=False)\n",
    "    print(\"Detailed Results Table:\")\n",
    "    print(detailed_df.to_string())\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['total_samples'] = total_samples\n",
    "    metrics['compiles'] = detailed_df['compiles'].str.startswith('Pass').sum()\n",
    "    metrics['correct_indentation'] = detailed_df['correct_indentation'].str.startswith('Pass').sum()\n",
    "    metrics['no_deprecations'] = detailed_df['no_deprecations'].str.startswith('Pass').sum()\n",
    "    metrics['correct_functionality'] = detailed_df['correct_functionality'].str.startswith('Pass').sum()\n",
    "\n",
    "\n",
    "\n",
    "    summary_score = (\n",
    "        metrics['compiles'] +\n",
    "        metrics['correct_indentation'] +\n",
    "        metrics['no_deprecations'] +\n",
    "        (3 * metrics['correct_functionality'])\n",
    "    )\n",
    "    metrics['summary_score'] = summary_score\n",
    "\n",
    "    summary_df = pd.DataFrame([metrics])\n",
    "    summary_df.to_csv(SUMMARY_METRICS_CSV, index=False)\n",
    "    print(\"Summary Metrics:\")\n",
    "    print(summary_df.to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
